#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•è¿è¡Œè„šæœ¬
"""
import os
import sys
import subprocess
import argparse
import time
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


def run_backend_performance_tests():
    """è¿è¡Œåç«¯æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ Running Backend Performance Tests...")
    
    # å®‰è£…æ€§èƒ½æµ‹è¯•ä¾èµ–
    subprocess.run([
        sys.executable, "-m", "pip", "install", "-r", 
        "backend/requirements-dev.txt"
    ], check=True)
    
    # è¿è¡Œ pytest-benchmark æµ‹è¯•
    cmd = [
        "pytest", 
        "backend/tests/performance/test_api_performance.py",
        "--benchmark-only",
        "--benchmark-sort=mean",
        "--benchmark-json=performance_results/backend_benchmark.json",
        "--benchmark-columns=min,max,mean,stddev,rounds,iterations",
        "-v"
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode == 0:
        print("âœ… Backend performance tests completed successfully")
        print(result.stdout)
    else:
        print("âŒ Backend performance tests failed")
        print(result.stderr)
        return False
    
    return True


def run_load_tests(host="http://localhost:8000"):
    """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
    print(f"ğŸ”¥ Running Load Tests against {host}...")
    
    # æ£€æŸ¥ Locust æ˜¯å¦å®‰è£…
    try:
        subprocess.run(["locust", "--version"], check=True, capture_output=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("Installing Locust...")
        subprocess.run([sys.executable, "-m", "pip", "install", "locust"], check=True)
    
    # è¿è¡Œå‹åŠ›æµ‹è¯•
    cmd = [
        sys.executable,
        "backend/tests/performance/stress_test_runner.py",
        "--host", host,
        "--test", "all"
    ]
    
    result = subprocess.run(cmd)
    
    if result.returncode == 0:
        print("âœ… Load tests completed successfully")
    else:
        print("âŒ Load tests failed")
        return False
    
    return True


def run_frontend_performance_tests():
    """è¿è¡Œå‰ç«¯æ€§èƒ½æµ‹è¯•"""
    print("ğŸ¨ Running Frontend Performance Tests...")
    
    # åˆ‡æ¢åˆ°å‰ç«¯ç›®å½•
    frontend_dir = project_root / "frontend"
    
    # å®‰è£…ä¾èµ–
    subprocess.run(["npm", "install"], cwd=frontend_dir, check=True)
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    test_commands = [
        # è¿è¡Œå•å…ƒæµ‹è¯•
        ["npm", "run", "test", "--", "--run"],
        # è¿è¡Œ Lighthouse CI (å¦‚æœé…ç½®äº†)
        # ["npx", "lhci", "autorun"],
        # æ„å»ºå¹¶åˆ†æåŒ…å¤§å°
        ["npm", "run", "build"],
    ]
    
    for cmd in test_commands:
        print(f"Running: {' '.join(cmd)}")
        result = subprocess.run(cmd, cwd=frontend_dir)
        
        if result.returncode != 0:
            print(f"âŒ Command failed: {' '.join(cmd)}")
            return False
    
    # åˆ†ææ„å»ºç»“æœ
    analyze_build_performance(frontend_dir)
    
    print("âœ… Frontend performance tests completed")
    return True


def analyze_build_performance(frontend_dir):
    """åˆ†ææ„å»ºæ€§èƒ½"""
    dist_dir = frontend_dir / "dist"
    
    if not dist_dir.exists():
        print("âš ï¸ Build directory not found, skipping build analysis")
        return
    
    print("\nğŸ“Š Build Performance Analysis:")
    
    # åˆ†ææ–‡ä»¶å¤§å°
    total_size = 0
    file_sizes = []
    
    for file_path in dist_dir.rglob("*"):
        if file_path.is_file():
            size = file_path.stat().st_size
            total_size += size
            
            if size > 100 * 1024:  # å¤§äº100KBçš„æ–‡ä»¶
                file_sizes.append((file_path.name, size))
    
    # æ’åºå¹¶æ˜¾ç¤ºå¤§æ–‡ä»¶
    file_sizes.sort(key=lambda x: x[1], reverse=True)
    
    print(f"Total build size: {total_size / 1024 / 1024:.2f} MB")
    print("\nLargest files:")
    for name, size in file_sizes[:10]:
        print(f"  {name}: {size / 1024:.1f} KB")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ€§èƒ½é—®é¢˜
    if total_size > 10 * 1024 * 1024:  # 10MB
        print("âš ï¸ Warning: Build size is quite large (>10MB)")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªå‹ç¼©çš„å¤§æ–‡ä»¶
    large_files = [f for f, s in file_sizes if s > 1024 * 1024]  # >1MB
    if large_files:
        print(f"âš ï¸ Warning: Found {len(large_files)} files larger than 1MB")


def setup_monitoring():
    """è®¾ç½®æ€§èƒ½ç›‘æ§"""
    print("ğŸ“ˆ Setting up Performance Monitoring...")
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = project_root / "performance_results"
    results_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºç›‘æ§é…ç½®
    monitoring_config = {
        "collection_interval": 30,
        "alert_rules": [
            {
                "name": "High CPU Usage",
                "metric": "cpu_usage",
                "threshold": 80,
                "duration": 300
            },
            {
                "name": "High Memory Usage", 
                "metric": "memory_usage",
                "threshold": 85,
                "duration": 300
            }
        ]
    }
    
    config_file = results_dir / "monitoring_config.json"
    with open(config_file, "w") as f:
        json.dump(monitoring_config, f, indent=2)
    
    print(f"âœ… Monitoring configuration saved to {config_file}")


def generate_performance_report():
    """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    print("ğŸ“‹ Generating Performance Report...")
    
    results_dir = project_root / "performance_results"
    
    if not results_dir.exists():
        print("âš ï¸ No performance results found")
        return
    
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "tests_run": [],
        "summary": {}
    }
    
    # æ”¶é›†æµ‹è¯•ç»“æœ
    for result_file in results_dir.glob("*.json"):
        try:
            with open(result_file) as f:
                data = json.load(f)
                report["tests_run"].append({
                    "file": result_file.name,
                    "data": data
                })
        except Exception as e:
            print(f"âš ï¸ Failed to read {result_file}: {e}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report_file = results_dir / "performance_report.json"
    with open(report_file, "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"âœ… Performance report saved to {report_file}")
    
    # ç”Ÿæˆ HTML æŠ¥å‘Š
    generate_html_report(report, results_dir / "performance_report.html")


def generate_html_report(report_data, output_file):
    """ç”Ÿæˆ HTML æ ¼å¼çš„æ€§èƒ½æŠ¥å‘Š"""
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Performance Test Report</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 40px; }}
            .header {{ background: #f5f5f5; padding: 20px; border-radius: 5px; }}
            .section {{ margin: 20px 0; }}
            .metric {{ background: #fff; border: 1px solid #ddd; padding: 15px; margin: 10px 0; }}
            .success {{ color: #52c41a; }}
            .warning {{ color: #faad14; }}
            .error {{ color: #ff4d4f; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>Performance Test Report</h1>
            <p>Generated: {report_data['timestamp']}</p>
        </div>
        
        <div class="section">
            <h2>Test Summary</h2>
            <p>Total tests run: {len(report_data['tests_run'])}</p>
        </div>
        
        <div class="section">
            <h2>Test Results</h2>
            {''.join([f'<div class="metric"><h3>{test["file"]}</h3><pre>{json.dumps(test["data"], indent=2)}</pre></div>' for test in report_data['tests_run']])}
        </div>
    </body>
    </html>
    """
    
    with open(output_file, "w") as f:
        f.write(html_content)
    
    print(f"âœ… HTML report saved to {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Run performance tests")
    parser.add_argument("--backend", action="store_true", help="Run backend performance tests")
    parser.add_argument("--frontend", action="store_true", help="Run frontend performance tests")
    parser.add_argument("--load", action="store_true", help="Run load tests")
    parser.add_argument("--host", default="http://localhost:8000", help="Target host for load tests")
    parser.add_argument("--all", action="store_true", help="Run all performance tests")
    parser.add_argument("--setup", action="store_true", help="Setup monitoring only")
    parser.add_argument("--report", action="store_true", help="Generate report only")
    
    args = parser.parse_args()
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("performance_results", exist_ok=True)
    
    success = True
    
    if args.setup:
        setup_monitoring()
        return
    
    if args.report:
        generate_performance_report()
        return
    
    if args.all or args.backend:
        success &= run_backend_performance_tests()
    
    if args.all or args.frontend:
        success &= run_frontend_performance_tests()
    
    if args.all or args.load:
        success &= run_load_tests(args.host)
    
    if not any([args.backend, args.frontend, args.load, args.all]):
        print("No tests specified. Use --help for options.")
        return
    
    # è®¾ç½®ç›‘æ§
    setup_monitoring()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_performance_report()
    
    if success:
        print("\nğŸ‰ All performance tests completed successfully!")
    else:
        print("\nğŸ’¥ Some performance tests failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()